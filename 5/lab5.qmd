---
title: "Lab 5"
subtitle: ""
author: "Thomas Haase"
date: last-modified
date-format: long
 
execute:
  warning: false
  echo: true
  error: false 
format: 
  pdf: 
    toc: true
    geometry:
     - top=2cm
     - left=2cm
     - right=2cm
     - bottom=3cm
    fig-pos: 'h'
    fig-cap-location: bottom
    tbl-cap-location: bottom
    include-in-header:
    - text: |
        \usepackage{placeins}
knitr:
  opts_chunk:
      fig.align: center
number-sections: false
#bibliography: citations.bib
editor_options: 
  chunk_output_type: console
---

---

# Part 1
In this lab, we will use a data set containing a random sample of public Facebook posts by members of the U.S. Congress from 2017.1 Our broad objective in this first part of the lab is to explore what topics were discussed, and possible variation by party membership. 

```{r}
#| label: load_libraries

library(quanteda)
library(topicmodels)
library(slam)

library(word2vec)

library(data.table)
library(kableExtra)

library(easystats)

library(tibble)
library(ggplot2)
library(tidytext)

setwd("~/Github/ML-Labs/5")
```

## Task 1

Begin by importing `fb-congress-data3.csv`. Report basic information about the data set; how many rows and column it has, as well as the name of the variables.

```{r}
#| label: Task1.1
#| fig.align: center
#| fig-width: 7
#| fig-height: 6

d <- read.csv("fb-congress-data3.csv")

tribble(
  ~Name,     ~Value,
  "Rows",    nrow(d) |> as.character(),
  "Columns", ncol(d) |> as.character()) |> 
  kable()

varnames <- names(d)
```

\textbf{Variables contained in the dataset:} `{r} varnames`


## Task 2

As you may have noticed from your inspection in #1, this data set has yet to be pre-processed (it contains punctuation, etc.). Hence, that is what you shall do now. More specifically, perform the following steps:


```{r}
#| label: Task1.2

# i
corp <- d |> 
  corpus(docid_field = "doc_id",
         text_field = "message",
         meta = list("screen_name",
                     "party"))

# ii
toks <- corp |>
  tokens(remove_punct = T,
         remove_numbers = T,
         remove_symbols = T,
         remove_url = T) |> 
# iii
  tokens_remove(stopwords("english"))

# iv
toks[1:3]
```

```{r}
#| label: Task1.2.2

# v
dfm <- toks |> dfm() |> 

# vi
  dfm_trim(min_termfreq = 5)
# only keep documents with more then 10 features
dfm <- dfm[which(rowSums(dfm)>=10), ]
```

## Task 3

Now we are ready to do some topic modeling! To do so, we will use the topicmodels package, and the function LDA(). Set x to your document-term-matrix and specify method="Gibbs" (note: Gibbs is the name of a particular estimation procedure; see the Appendix of the lecture for more details). Set the number of iterations to 1000, and specify a seed number to ensure replicability (hint: to specify iterations and seed number, use the control argument). Finally, set the number of topics, K=50 With these settings specified, start the estimation. This could take a minute or two.

```{r}
#| label: Task1.3

set.seed(5)

K <- 50 

if(!file.exists("lda.rds")){
  lda <- LDA(x = dfm, k = K,
             method="Gibbs",
             control=list(iter = 1000,
                          seed = 5,
                          verbose = 1))
  saveRDS(lda, file = "lda.rds")
} else {
  lda <- readRDS("lda.rds")
}
```


## Task 4
Once the estimation is finished, use the `get_terms()` function to extract the 15 words with the highest probability in each topic. In a real research setting, we would carefully examine each of the topics. Here, I only ask you to briefly skim them, and then focus on 5 that (i) you think are interesting, (ii) has a clear theme, and (iii) are clearly distinct from the other topics. Provide a label to each of those based on the top 15 words.
Complementing your label, please also provide a bar chart displaying on the y-axis the top 15 words, and on the x-axis their topic probabilities. Hint: you can retrieve each topic’s distribution over words using topicmodels’s function posterior.3 Lastly, please also report a general assessment—based on your skim—about the general quality of the topics; do most of them appear clearly themed and distinct, or are there a lot of “junk” topics?

```{r}
#| label: Task1.4
#| fig.align: center
#| fig-width: 7
#| fig-height: 6

# get_terms(lda,15)

words <- data.table(topic = 1:K,
                    posterior(object = lda)$terms) |> 
  melt.data.table(id.vars = 'topic')
words <- words[order(value,decreasing = T)]

t15 <- words[,head(.SD,15),by='topic']
t15 <- t15[topic %in% c(2,7,8,9,17)]

# plot
topic_labels <- c(
  "2"  = "Topic 2:\nForeign Security",
  "7"  = "Topic 7:\nHealthcare",
  "8"  = "Topic 8:\nEducation",
  "9"  = "Topic 9:\nCrisis",
  "17" = "Topic 17:\nInfrastructure"
)

# Apply topic labels
t15[, topic_label := topic_labels[as.character(t15$topic)]][!is.na(topic_label)] |> 
  ggplot(aes(x = value, y = reorder_within(variable, value, topic_label))) + 
  geom_bar(stat = 'identity', position = 'dodge', fill = "steelblue3") + 
  facet_wrap(~topic_label, scales = 'free', ncol = 2) +
  scale_y_reordered() +
  labs(title = "Top 15 Words of Selected Topics",
       x = "Probability", y = "") +
  theme_minimal()
```

There are a lot of junk topics, but a few of them are very distinct. 

## Task 5

Out of the 5 topics that you labeled, select two which you think are particularly interesting. For these two, identify the three documents which have the highest proportion assigned of this topic (hint 1: use topicmodels‘s `posterior()` to extract documents’ distribution over topics | hint 2: to identify the document ids which correspond to each row of what you extract from posterior(), you can use `ldaobject@documents`. See help file for more details.), and do a qualitative inspection ($= 2 \times 3$ documents to read). Does your readings corroborate your labels? Are they about what you expected?

```{r}
#| label: Task1.5
#| fig.align: center
#| fig-width: 7
#| fig-height: 6

documents <- data.table(doc_id = lda@documents,
                        posterior(object = lda)$topics)
# Assign topics as column names
colnames(documents)[2:ncol(documents)] <- paste0('Topic',colnames(documents)[2:ncol(documents)]) 



top_docs <- documents[
  order(Topic8,decreasing = T)][
    1:3,doc_id] |> 
  as.integer()

tibble(ID = as.character(top_docs),
       Education = as.character(corp)[top_docs]) |> 
  kable() |> 
  kable_styling(bootstrap_options = c("striped", "hover", 
                                      "condensed", "responsive")) |>
  column_spec(1, bold = TRUE, width = "3em") |>
  column_spec(2, width = "40em")
```

```{r}
#| label: Task1.5.2
#| fig.align: center
#| fig-width: 7
#| fig-height: 6

top_docs <- documents[
  order(Topic17,decreasing = T)][
    1:3,doc_id] |> 
  as.integer()

tibble(ID = as.character(top_docs),
       Infrastructure = as.character(corp)[top_docs]) |> 
  kable() |> 
  kable_styling(bootstrap_options = c("striped", "hover", 
                                      "condensed", "responsive")) |>
  column_spec(1, bold = TRUE, width = "3em") |>
  column_spec(2, width = "40em")
```

For both topics the three selected prototypical-tweets adress gunviolence and infrasstructure. The model seems very convincing, since the content of the inspected texts meet my expectation.


## Task 6

Now, estimate a topic model—as in #3—but with K=3 instead. Extract the top 15 words from each topic, (try to) label each, and then make an assessment of the overall quality of them. To further explore the quality of this topic model, reconsider the documents you read in #5: extract the distribution over topics for these documents (from your new K=3 model). How well does this topic model capture the theme of these documents? Based on your analysis, which of the two K’s do you prefer? Motivate.

```{r}
#| label: Task1.6
#| fig.align: center
#| fig-width: 7
#| fig-height: 6

# estimate LDA
set.seed(5)
K <- 3

if(!file.exists("lda2.rds")){
  lda <- LDA(x = dfm, k = K,
             method="Gibbs",
             control=list(iter = 1000,
                          seed = 5,
                          verbose = 1))
  saveRDS(lda, file = "lda2.rds")
} else {
  lda <- readRDS("lda2.rds")
}


# get top words
get_terms(lda,15) |> 
  kable(col.names = c("Healthcare & Tax", 
                      "Veterans & Community", 
                      "National Government"),
        caption = "Top 15 Terms of Topic Model (K = 3)")
```

It seems like multiple topics got recognized as one from the model. There are probably more then 3 larger topics adressed in the texts.

```{r}
#| label: Task1.6.2
#| fig.align: center
#| fig-width: 4
#| fig-height: 3

documents <- data.table(doc_id = lda@documents,
                        posterior(object = lda)$topics)
# Assign topics as column names
colnames(documents)[2:ncol(documents)] <- paste0('Topic',colnames(documents)[2:ncol(documents)]) 

set.seed(5)

documents[,-"doc_id"] |> 
  as.matrix() -> mat

par(mar=c(5,10,4,2))
mat[order(apply(mat, 1, which.max)), ] |> 
  heatmap(Rowv = NA, Colv = NA, margins = c(10, 1))
```

The heatmap, that ordered the documents by maximum probability, shows, that the distribution over Topics is quite distinct. Only few texts have high probability in all topics. The K= 50 model captured unnecessary topics, while the K = 3 one still shows many stripes in its heatmap. These could probably make up new topics! The optimum lies somewhere inbetween 3 and 50. Just from the Interpretation the 50 topic model is more powerful because it captures more distinctions. 


## Task 7
Continuing with the topic model you concluded the most appropriate, perform the following sets of analyses: 

- i. Compute the prevalence of each topic, across all documents. Report which is the most prevalent topic, overall, and then report—in the form of a single plot; e.g., a bar chart—the prevalence of the topics you labeled. 

-ii. Compare the prevalence on your labeled topics between democrats and republicans. You can for example fit a fractional regression model using glm(family="quasibinomial") or using t-tests of difference in means. Interpret.

```{r}
#| label: Task1.7
#| fig.align: center
#| fig-width: 7
#| fig-height: 6

# load 50 topic lda
lda <- readRDS("lda.rds")
K <- 50


documents <- data.table(doc_id = lda@documents,
                        posterior(object = lda)$topics)
colnames(documents)[2:ncol(documents)] <- paste0('Topic',colnames(documents)[2:ncol(documents)]) 

# query for most prevalent topic
top_topic <- documents[, -"doc_id"] |>
  colSums() |> 
  which.max() |> 
  names()

# report
cat("Most prevalent topic:", top_topic, "\n")
```


```{r}
#| label: Task1.7.2
#| fig.align: center
#| fig-width: 6
#| fig-height: 3

# plot labled topics
data.table(Topic = documents[, -"doc_id"] |>
             colSums() |>
             names(),
           Prevalence = documents[, -"doc_id"] |> 
             colSums())[
               sub("Topic", "", Topic) %in% names(topic_labels)
               ][
                 , Topic := topic_labels[sub("Topic", "", Topic)] 
                 ] |> 
ggplot(aes(x = reorder(Topic, Prevalence), 
           y = Prevalence)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(
    x = "Prevalence", y = "",
    title = "Barchart of Topicprevalences"
  ) +
  scale_y_continuous(breaks = seq(0, 150, 10)) +
  theme_minimal()
```


```{r}
#| label: Task1.7.3

# prepare data 
documents <- data.table(
  doc_id = lda@documents,
  posterior(lda)$topics
) 
colnames(documents)[2:ncol(documents)] <- paste0('Topic',colnames(documents)[2:ncol(documents)]) 
# add partyinformation
documents <- documents[, party := docvars(dfm)$party |> 
                         factor(levels = c("Democrat",
                                           "Republican"))]
# filter data
documents <- documents[, .(doc_id,party,
                           Topic2,Topic7,
                           Topic8,Topic9,
                           Topic17)]
setnames(documents, 
         c("Topic2","Topic7","Topic8",
           "Topic9","Topic17"), 
         c("Foreign Security",
           "Healthcare",
           "Education",
           "Crisis",
           "Infrastructure"))

# calculate model
glm <- glm(party ~ `Foreign Security` + Healthcare + 
             Education + Crisis + Infrastructure,
           family = "binomial",
           data = documents)

# because quasibinomial was suggested but 
# Im using binomial lets check overdispersion
check_overdispersion(glm)
```


```{r}
#| label: Task1.7.4
#| fig.align: center
#| fig-width: 6
#| fig-height: 3

glm |> 
  parameters(exponentiate = F) |> 
  plot()

glm |> summary()
```

The topics Foreign Security, Healthcare, Education and Infrastructure can distinguish between both parties statistically significant. Adressing the topic "Crisis" can not be used to distinguish between parties. 


## Task 8

BONUS (not obligatory; suggestion; do after you have completed the rest of the lab). As a bonus exercise—to expose you to the traditional computer sciency way of selecting the number of topics, K-you shall consider a data-driven approach, relying on the measure of hold-out likelihood (or, perplexity as its also called). To do so, do the following:

- i. Split your document term matrix into two (a training and test set); 80/20 division.

- ii. Write a loop which in each iteration (a) estimates a topic model using a particular K, and then (b) computes (and stores) its perplexity using the topicmodels function perplexity(), which takes as input the model object and the test document-term-matrix (note: the document-term-matrix needs to be transformed into a particular format: use ... for this).

- iii. Consider the following range of K: ${3, 10, 25, 50, 75, 100, 200}$, and run the loop. This may take a few minutes. Once the loop has finished, plot your results (x-axis: K, y-axis: perplexity).


Interpret. Based on this, what is a reasonable K?

```{r}
#| label: Task1.8.1

rm(documents, lda, mat, t15, words, toks, corp,
   K, top_docs, top_topic, topic_labels, varnames)

# Split data
set.seed(5)
train_ids <- sample(seq_len(nrow(dfm)), size = floor(0.8 * nrow(dfm)))
train_data <- dfm[train_ids, ]
test_data <- dfm[-train_ids, ]
```

```{r}
#| label: Task1.8.2

ks <- c(3, 10, 25, 50, 75, 100, 200)
perp <- c()

if(!file.exists("perplexity.rds")){
  
  for(i in 1:length(ks)){
  
    # Train
    current_tm <- LDA(x = train_data,
                      k = ks[i],
                      method="Gibbs",
                      control=list(iter = 500,
                                   seed = 1,
                                   verbose = 100))
    # Compute perplexity
    perp[i] <- perplexity(object = current_tm,
                          newdata = as.simple_triplet_matrix(test_data)) # For some reason, perplexity() wants the test data in simple_triplet_matrix format.
    print(perp)
  }
  
  saveRDS(perp, file = "perplexity.rds")
} else {
  perp <- readRDS("perplexity.rds")
}



par(mar=c(5,4,4,2)+0.1)
plot(x=ks,y=perp,
     type="b",
     xlab="n Topics",
     ylab="Perplexity",
     main="Perplexity vs. Topic Model Topiccount")
```

Perplexity measures how well LDA models predict unseen documents, with lower scores indicating better performance and topic understanding. It measures how well the model predicts unseen or held-out documents. Since perplexity is lowest at 100 topics the facebookposts can statistically optimal be clustered into 100 topics. In a previous task it was already detected, that a K of 50 topics already contain a lot of uninterpretable ones, which is why 100 topics might be statistically optimal but useless to human interpreters. 


# Part 2
In this second part of the lab, we will continue with the data U.S. Congress–Facebook posts data set. However, now with a different focus: a focus on the word-level, using word embeddings instead of topic models.
```{r}
rm(list = ls())
```


## Task 1
Because word embeddings are not negatively affected by stop words or other highly frequent terms, your first task is to reimport the `fb-congress-data3.csv` file, and re-process the data; performing step i–ii in task #2, but skipping #3. Here, we also do not want to transform our documents into a document-term matrix. Instead, after having tokenized and cleaned the documents, paste each back into a single string per document. Hint: for this, you could for example write: sapply(mytokens,function(x)paste(x,collapse = " ")). As a last pre-processing step, transform all your text into lowercase (hint: you can use the function tolower() for this).

```{r}
#| label: Task2.1

d <- read.csv("fb-congress-data3.csv")

# i
corp <- d |> 
  corpus(docid_field = "doc_id",
         text_field = "message",
         meta = list("screen_name",
                     "party"))
# ii
toks <- corp |>
  tokens(remove_punct = T,
         remove_numbers = T,
         remove_symbols = T,
         remove_url = T)

s <- sapply(toks,function(x)paste(x,collapse = " ")) |> 
  tolower()
```

## Task 2
Now we are set to fit word embeddings! To begin, let us fit one word embedding model to all documents—not separating posts by democrats and republicans. Use word2vec’s word2vec() function to fit a cbow model (type="cbow") using 15 negative samples per real context/observation (negative=15), and setting dim=50, the number of dimensions of the word vetors/embeddings. This will take a minute or two.

```{r}
#| label: Task2.2

set.seed(5)

we <- word2vec(x = s, type = "cbow", 
               iter = 50, hs = T, 
               negative = 15, 
               threads = 4)

```


## Task 3
When the estimation in #2 is finished, identify the 10 nearest terms to 3 focal words of your choice/interest. Make sure to select words which occur frequently in your data. Hint: to retrieve the closest words in embedding/word vector space, you may use the following code: `predict(w2v,c("word2","word2","word3"),type="nearest",top_n = 10)`, where wv2 is the object storing the fitted model of the word2vec function. Does the results you find makes sense? Why/why not?

```{r}
#| label: Task2.3

nearw <- predict(we, c("trump",
                       "food",
                       "russia"),
                 type = "nearest",
                 top_n = 10)



nearw$trump |> kable()
```

The term "trump" has names of other leaders and leadingpositions in its nearby area. This makes intuitive sense, because trump was president in the year when the facebookposts were collected.

```{r}
#| label: Task2.3.1
nearw$food |> kable()
```

The term "food" is surrounded by other terms that are crisis, infrastructure and ressource related. This makes intuitive sense, because food just as infrastructure is something that should be made always available by the politics in its society-steering role. Also all terms are crisis related since a crisis is often related with situations were food, ports, stores, ... is not available. Some nonsensical numbers are also in there, aswell as "comment".


```{r}
#| label: Task2.3.2
nearw$russia |> kable()
```

The term "russia" is surrounded by synonyms/similar terms, aswell as "campaign","mueller", "elections", "extend" and "meddling". 
Since russian interference in the US votings was a big topic in 2017 these associations are no s urprise. The resulting FBI investigation put pressure on trump, which is why he fired the FBI director and Robert Mueller was appointed to oversee the investigation.

## Task 4

What initially made people so excited about word embeddings was their surprising ability to solve seemingly complex analogy tasks. Your task now is to attempt to replicate one such classical analogy result, first with the embedding vectors that you have already estimated, and second using a pre-trained embedding model. To do so, please perform the following steps:

- i. Extract the whole embedding matrix: embedding <- as.matrix(w2v).

- ii. Identify the rows in the embedding matrix which correspond to king, man, woman, and create a new R object kingtowoman which is equal to the vector for king, minus the vector for man, plus the vector for woman. Hint: to extract the row corresponding to a particular word (e.g., “king”), you may use w2v[rownames(w2v)=="king",].

- iii. Use word2vec’s function word2vec_similarity() to identify the 20 most similar words to kingtowoman. Do you find “queen” in the top 20? Why do you think you get the result you do? 

- iv. Next, we will consider a pre-trained embedding model (trained on all Wikipedia articles that existed in 2014 and about 5 million news articles). The embedding vectors from this model are stored in the file `glove6B200d.rds` Note: this file is large; more than 300MB. Use readRDS() to import it, and stored it in an R object called pretrained. Each row stores the embedding vector for a particular word. With this info in mind, report how many embedding dimensions were used for this model, and how many words we have embedding vectors for.

- v. Repeat steps ii–iii for pretrained. Does “queen” appear in the top 20 here? What do you think explains this difference/similarity to the self-trained result?
```{r}
#| label: Task2.4

# i.
e <- as.matrix(we)

# ii.
KING  <- e[rownames(e)=="king",]
MAN   <- e[rownames(e)=="man",]
WOMAN <- e[rownames(e)=="woman",]

kingtowoman <- KING - MAN + WOMAN

# iii
kingtowoman <- matrix(kingtowoman, nrow = 1)
rownames(kingtowoman) <- "kingtowoman"

word2vec_similarity(kingtowoman, e, 
                    top_n = 20, 
                    type = "cosine") |> 
  kable()
```

Looks like "queen" is not among the closest 20 terms!

```{r}
#| label: Task2.4.1

e[rownames(e)=="queen",]
```

The reason is because queen is not part of the embeddingspace! Therefore we can only approximate where it should be but will never find it as an observation in the space.

```{r}
#| label: Task2.4.2

# iv.
pretrained <- readRDS("glove6B200d.rds")

nrow(pretrained)
ncol(pretrained)
#pretrained[50:53,1:3]
```

The embeddingmodel contains 400000 words each with a vector of size 200.

```{r}
#| label: Task 2.4.3

# v.

KING  <- pretrained[rownames(pretrained)=="king",]
MAN   <- pretrained[rownames(pretrained)=="man",]
WOMAN <- pretrained[rownames(pretrained)=="woman",]

kingtowoman <- KING - MAN + WOMAN

kingtowoman <- matrix(kingtowoman, nrow = 1)
rownames(kingtowoman) <- "kingtowoman"

word2vec_similarity(kingtowoman,
                    pretrained, 
                    top_n = 20, 
                    type = "cosine") |> 
  kable()
```

Queen does indeed appear as the second term in the embeddingspace. Most of the terms appearing in the list have something to do with "king". The terms in the self-trained list deviated more since the corpus of the self trained embeddings does not contain so many words related to "king". 

- vi. Given the result in (v), what do you expect, if you were to construct a measure of occupational gender bias along the lines of Garg et al. (2018), that is by comparing the distance between different occupations and gendered words, for example: occupationalbias = dist(statistician, man) – dist(statistician, woman), would this score be “more correct” than the one you would obtain from the same calculation on your facebook/congress model? Why/why not?

What is correct depends a lot on the question and context. In general the wikipediacorpus is a larger and more general dataset which could be interesting to research because of that. The genderbias vector in this wikipediadataset would then be a "very correct" measure of itself. On the other hand in our own facebook dataset a similar question would be possible to answer in the same way. Since the data is more sparse/smaller its harder to trust the results. During the calculation the words place themselves in relation to all other words. If there are less words the uncertainty of its position is generally higher.


## Task 5 

Now we shall make a comparison between democrats and republicans. Split the data from step #1 into two based on party affiliation. Then, repeat 2–3, but now separately for republicans and democrats. For #3, select words which you expect might be used differently between the two political camps (but still are frequently used by both; for example “abortion”, “obamacare”). Do you find any differences? Do they align with your expectations?

```{r}
#| label: Task2.5

# Split data
toks_dem <- toks[which(toks$party == "Democrat"), ]
toks_rep <- toks[which(toks$party == "Republican"), ]

s_dem <- sapply(toks_dem, function(x)paste(x,collapse = " ")) |> 
  tolower()
s_rep <- sapply(toks_rep, function(x)paste(x,collapse = " ")) |> 
  tolower()


# word2vec
set.seed(5)

we_dem <- word2vec(x = s_dem, 
                   type = "cbow",
                   iter = 50, hs = T,
                   negative = 15,
                   threads = 4)

we_rep <- word2vec(x = s_rep, 
                   type = "cbow",
                   iter = 50, hs = T,
                   negative = 15,
                   threads = 4)

# get nearby words
nw_dem <- predict(we_dem,
                  c("trump",
                    "food",
                    "russia"),
                  type = "nearest",
                  top_n = 10)

nw_rep <- predict(we_rep,
                  c("trump",
                    "food",
                    "russia"),
                  type = "nearest",
                  top_n = 10)


nw_dem$trump |> kable()
nw_rep$trump |> kable()
```

Both in the republicans and democrat wordembedding "trump" appears in the recion of obama, vice and other leaders. 

```{r}
nw_dem$food |> kable()
nw_rep$food |> kable()
```

Food is connected to crisis and life related terms for both parties. 

```{r}
nw_dem$russia |> kable()
nw_rep$russia |> kable()
```

For the term "Russia" a meaningful difference in framing is observable. Democrats associate russia with more negative terms like obstruction, or question credibility. Also the "mueller" incident discussed in a previous task is appearing here. Republicans associate russia with terms regarding the war, but not necesarry negativly. imposing, unleaching, efficiency is a neutral and almost a careful positive framing.

## Task 6 
```{r}
#| label: Task2.6
rm(d,pretrained,corp,KING,MAN,WOMAN)

# read wordlists
pos <- readLines("positive.txt")
neg <- readLines("negative.txt")

# get embedding matrices
e_dem <- as.matrix(we_dem)
e_rep <- as.matrix(we_rep)


# Create the projection:
# 1) Extract the relevant word vectors from the "embedding" matrix and 
# compute averages
posv_dem <- e_dem[which(rownames(e_dem) %in% pos),]|> 
  apply(2,mean) |> 
  as.matrix() |> 
  t()
negv_dem <- e_dem[which(rownames(e_dem) %in% neg),]|> 
  apply(2,mean) |> 
  as.matrix() |> 
  t()

posv_rep <- e_rep[which(rownames(e_rep) %in% pos),] |> 
  apply(2,mean) |> 
  as.matrix() |> 
  t()
negv_rep <- e_rep[which(rownames(e_rep) %in% neg),] |> 
  apply(2,mean) |> 
  as.matrix() |> 
  t()

# 2) Compute the difference to get the dimension
neg_pos_dem <- negv_dem - posv_dem
neg_pos_rep <- negv_rep - posv_rep

neg_assoc_terms_dem <- word2vec::word2vec_similarity(x = neg_pos_dem, 
                                                 y = e_dem[-which(rownames(e_dem) %in% pos),],
                                                 top_n = 10000) # get distance to all words
neg_assoc_terms_rep <- word2vec::word2vec_similarity(x = neg_pos_rep, 
                                                 y = e_rep[-which(rownames(e_rep) %in% pos),],
                                                 top_n = 10000) # get distance to all words
  
neg_assoc_terms_dem[,-1] |> 
  head(50) |> 
  kable()

neg_assoc_terms_rep[,-1] |> 
  head(50) |> 
  kable()


word_negativity <- function(df,word){
  df[which(df$term2 == word),]
}
```


```{r}
word_negativity(neg_assoc_terms_dem, "obama")$rank
word_negativity(neg_assoc_terms_rep, "obama")$rank
```

"Obama" is more negativly associated in the republican embedding space compared to the democrats embedding space ("obama" has a higher rank on the constructed negativity "scale"). 

```{r}
word_negativity(neg_assoc_terms_dem, "russia")$rank
word_negativity(neg_assoc_terms_rep, "russia")$rank
```

"Russia"s rank is almmost similar for both parties, but the republicans are a bit more negative about it.

```{r}
word_negativity(neg_assoc_terms_dem, "war")$rank
word_negativity(neg_assoc_terms_rep, "war")$rank
```

"War" has a much higher negativity rank in the republican wordembedding model. Are democrats in favour of war? Probably not. 

```{r}
kwic(toks_dem, "war", window = 5) |> kable()
```

A quick keyword in context (kwic) analysis shows that democrats honor heroes and vctims of war, aswell as criticising republican war involvement.
