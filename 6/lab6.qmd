---
title: "Lab 6"
subtitle: ""
author: "Thomas Haase"
date: last-modified
date-format: long
 
execute:
  warning: false
  echo: true
  error: false
   
format: 
  typst:
    toc: true
    papersize: a4
    fig-align: center
    margin:
      left: 2cm
      right: 2cm
      top: 2cm
      bottom: 3cm
    columns: 1

editor_options: 
  chunk_output_type: console
---

---

# Part 1: Meta-learners for job training evaluation

The dataset "job_training_updated.csv" contains information about 12,000 individuals who either participated or did not participate in a job training program, including:
- training: Binary indicator of whether individual participated in training (treatment)
- earnings: Post-training annual earnings in thousands of dollars (outcome)
- age: Age of individual
- education: Years of education
- prior_earnings: Earnings before training program
- employment_history: Years of prior employment
- urban: Binary indicator of urban residence

```{r}
#| label: Setup Environment

library(easystats)
library(data.table)
library(kableExtra)

library(rpart)
library(rpart.plot)


library(randomForest)

library(caret)

library(htetree)
library(grf)

set.seed(5)

setwd("~/Github/ML-Labs/6")
```

## Task 1
1. Fit regular OLS regression using lm(), including all non-treatment and non-outcome variables as control variables. Interpret the coefficient for the treatment variable as the average treatment effect. Considering what we talked about in the lecture, what properties of the data would lead you to believe your estimate is biased? Motivate.

```{r}
#| label: Task 1.1

d <- data_read("job_training_updated.csv")

m1 <- lm(earnings ~ training + age + education + 
         prior_earnings + employment_history + urban, d) 

m1 |> report_table() |> summary() |> print_md()
```

The ATE of the treatment variable `training` on earnings is very large. Despite the statistical significance the model could be improved by causal modeling. The standard paradigm is problematic because it lacks assumes linearity. In case a confounder is non-linear the estimate will be biased.

## Task 2
Next, you shall estimate an orthogonal learner, using decision trees as the method for predicting both the treatment and the outcome. Please follow the following steps:

- a. Train a decision tree model using rpart() to predict training from all confounders using the full dataset. For classification trees, use method="class" and for the control parameters use: cp=0, minbucket=5, maxdepth=30
 (i.e.,  control=rpart::rpart.control(cp=0,minbucket=5, maxdepth=30)).
 
- b. Train a decision tree model using rpart() to predict earnings from all confounders using the full dataset. For regression trees, use method="anova" and the same control parameters: cp=0, minbucket=5, maxdepth=30  (i.e.,  control=rpart::rpart.control(cp=0,minbucket=5, maxdepth=30)).

- c. Make predictions of treatment (using model from a, with type="prob") and outcome (using model from b) for all observations.

- d. Calculate residuals for all observations: X_tilde = X - X_hat, Y_tilde = Y - Y_hat.

- e. Estimate the ATE by regressing Y_tilde on X_tilde using lm().

- f. Report the ATE. How does it compare to your OLS estimate in #1?

- g. Which of the two methods do you trust more? Can you think of any aspect of the implementation of the orthogonal learner which could bias its estimate?


```{r}
#| label: Task 1.2

# a
m_X <- rpart(training ~ age + education + prior_earnings +
               employment_history + urban, d, method = "class",
             control = rpart.control(cp = 0, minbucket = 5,
                                     maxdepth = 30)) 
# b
m_Y <- rpart(earnings ~ age + education + prior_earnings +
               employment_history + urban, d, method = "anova",
             control = rpart.control(cp = 0, minbucket = 5,
                                     maxdepth = 30)) 
# c
X_hat <- predict(m_X, newdata = d[3:7], type="prob")[,"1"]
Y_hat <- predict(m_Y, newdata = d[3:7])

# d
residuals <- data.frame(X = d$training - X_hat,
                        Y = d$earnings - Y_hat)

# e
m_ate <- lm(Y ~ X, residuals)

# f
m_ate |> parameters() |> print_md()
m_ate |> report_parameters(include_intercept = F)
```


## Task 3

Given your conclusions in 2, do you think either of the following two changes to the setup of the orthogonal learner could improve the ATE estimate? (i) switching from a decision tree to a random forest, (ii) add cross-fitting. Motivate.

Cross-fitting means to predict out-of-fold to block contamination whereby the orthogonal learner does not bias the residuals in hold-out towards 0. To adress the high variance of the trees by prohibiting the most important variables to dominate the trees. The negative aspect of forests that they are less interpretable is not relevant here since we are only interested in predicting the effect of the confounders to X and Y. 


## Task 4

Now you shall implement the two updates discussed in 3. Please do the following:

 a. Divide your data into 5 folds (hint: you can use createFolds() from the caret package).
 b. Create a for-loop which in each iteration i does the following:
    i. Train a random forest model using randomForest() (with ntree=200 and mtry=2) predicting training from confounders on data in folds = i.
    ii. Train a random forest model using randomForest() (with ntree=200 and mtry=2) predicting earnings from confounders on data in folds = i.
    iii. Use models from (i) and (ii) to predict treatment (with type="prob") and outcome for ob- servations in fold i.
    iv. Calculate residuals X_tilde and Y_tilde for observations in fold i.
    v. Store residuals from fold i.
 c. Combine dataset of residuals and regress Y_tilde on X_tilde using lm().
 d. Report the estimated ATE. Do you trust this estimate more than those in 2, and if so why (or why not)?
 
 
```{r}
#| label: Task 1.4

if(file.exists("Task_1_4.rds")){
  residuals <- readRDS("Task_1_4.rds")
} else{

  # a
  ids_folds <- createFolds(d$earnings, k = 5)
  residuals_list <- list()  
  
  # b
  for(i in seq_along(ids_folds)){
    
    ids_test <- ids_folds[[i]]
    
    cat("------------------------------\n
        Start Iteration", i, "\nSplitting Data\n")
    
    testdata <- d[ids_test,] |> 
      as.data.frame() |> 
      data_select(c("age","education","prior_earnings",
                    "employment_history","urban",
                    "training","earnings"))
    
    trainingdata <- d[-ids_test,] |> 
      as.data.frame() |> 
      data_select(c("age","education","prior_earnings",
                    "employment_history","urban",
                    "training","earnings"))
    
    confounders <- c("age","education","prior_earnings",
                    "employment_history","urban")
    
    # i
    cat("Calculate RF for X\n")
    m_X <- randomForest(x = trainingdata[,confounders],
                        y = trainingdata$training,
                        ntree = 200, mtry = 2) 
    # ii
    cat("Calculate RF for Y\n")
    m_Y <- randomForest(x = trainingdata[,confounders],
                        y = trainingdata$earnings,
                        ntree = 200, mtry = 2) 
    # iii
    cat("Predict Testdata\n")
    X_hat <- predict(m_X, newdata = testdata[,confounders])
    Y_hat <- predict(m_Y, newdata = testdata[,confounders])
    
    # iv
    cat("Store Residuals\n")
    residuals_list[[i]] <- data.frame(
      fold = i,
      original_index = ids_test,
      X = testdata$training - X_hat,
      Y = testdata$earnings - Y_hat
    )
  }
  
  residuals <- do.call(rbind, residuals_list)
  residuals <- residuals[order(residuals$original_index), ]  
  
  saveRDS(residuals,"Task_1_4.rds")
}


# c
m_ate <- lm(Y ~ X, residuals)

# f
m_ate |> parameters() |> print_md()
m_ate |> report_parameters(include_intercept = F)
```
 
I trust this estimate more then the ones before since random forest prevents overfitting the ATE is not biased towards 0 anymore. Because of that the RF ATE is a bit larger then the decisiontree ATE. 


## Task 5

Suppose we learn that the true average treatment effect is 5.5 thousand dollars. Report which method
came closest, and discuss what this says about the properties of the data—in particular the relation
between the confounders and the treatment and outcome. 
 
The used models assume all confounding variables are included in the model. Since the true ATE is so different from the estimated ones not all confounding variables were included in the model. This points the researcher towards theorybuilding :) 

```{=typst}
#pagebreak()
```

# Part 2: Heterogeneity I

The dataset "scholarship.csv" contains information about 15,000 students who either received or did not receive a college scholarship, including:
- scholarship: Binary indicator of scholarship receipt (treatment)
- completed: Binary indicator of degree completion within 6 years (outcome)
- gpa: High school GPA (scale 0-4)
- parental_income: Parental income in thousands of dollars
- first_generation: Binary indicator of first-generation college student status
- sat_score: SAT score (scale 400-1600)
- distance_to_college: Distance from home to college in miles
- financial_need: Measure of financial need (scale 0-100)

```{r}
#| label: Setup Part 2

rm(list = ls())

d <- data_read("scholarship.csv")
```


## Task 1
Suppose your co-author, who has done a careful literature review, has found support for two of the variables, first_generation and financial_need, having a moderating effect. What you shall do first is examine whether you find evidence of this in your data. Implement a standard linear regression using lm() (or glm() if you prefer logistic regression) with the treatment variable as well as all other input variables (presumed confounders) included, with first_generation and financial_need interacted with the treatment variable. Report your findings: do you find evidence supporting your colleague’s conclusion from the literature?

```{r}
#| label: Task 2.1

m1 <- lm(completed ~ scholarship + gpa + parental_income +
         first_generation + sat_score + distance_to_college + financial_need +
         scholarship:first_generation + scholarship:financial_need, 
         data = d)

m1 |> report_table() |> summary() |> print_md()
m1 |> parameters() |> plot(show_intercept = T)
```

My co-author is partly right: 
- "first generation" has a statistically significant positive interaction effect
- "financial need" has a statistically significant positive interaction effect, but it is so tiny that the significance is probably due to the large observation size. It does not seem to exist.


## Task 2

Considering what we discussed in the lecture, what is one limitation of this standard approach to effect heterogeneity? What are properties of the data (or state of the field) that could make this limitation more or less problematic?

Research frequently find that the effect of events, exposures, policies (etc) vary across individuals. Usually interactionterms are implemented in the model with the selection of subgroups being based on theory or convention. Limitations of this procedure are that it assumes we know moderators beforehand. While exploratory analysis could help it creates risk of p-hacking and is not feasable for data with too many variables.

## Task 3
Next, you shall consider an alternative approach to effect heterogeneity, using causal trees. At a high level, describe what is the key difference in assumption we make when using causal trees compared to the traditional approach?

Causal trees identify the splits that maximize across-leaf variation in the within-leaf treated–untreated outcome difference. Standard trees just minimize the variance in the leaves. Using trees solves the problem above since they include the most relevant interactions by design.


## Task 4
Perform a causal tree analysis by doing the following:

a. Estimate the causal tree using the function causalTree() from the htetree package, specifying the formula as in #1 (except drop the interactions and leave out the treatment variable; the latter is specified separately). Use the following parameters: split.Rule="CT", cv.option="CT", split.Honest=TRUE, split.Bucket=TRUE, minsize=60, cp=0, bucketNum=40.

b. Visualize the tree using rpart.plot() and describe the combination of splits which identify the population with (a) the largest treatment effect and (b) the smallest.

c. Suppose our dataset is a standard observational dataset common to the social sciences, e.g., a survey dataset of a random sample of the population. Given this information, what could be a potential threat to the validity of our causal tree results?

```{r}
#| label: Task 2.4
#| fig.align: center
#| fig-width: 15
#| fig-height: 10
#| fig-caption: "Causal Tree"

# a. 
set.seed(5)
if(file.exists("task2_4.rds")){
  m2 <- readRDS("task2_4.rds")
  } else {
    
    m2 <- causalTree(
      completed ~ gpa + parental_income +
             first_generation + sat_score + distance_to_college + financial_need,
      data      = d,
      treatment = d$scholarship,
      split.Rule = "CT", cv.option = "CT", 
      split.Honest = T, cv.Honest = T, split.Bucket = T,  
      minsize = 60, cp = 0, bucketNum = 40
    )
    
    saveRDS(m2, "task2_4.rds")
}

# m2 <- causalTree(
#   completed ~ gpa + parental_income + first_generation +
#     sat_score + distance_to_college + financial_need,
#   data      = d,
#   treatment = d$scholarship,
#   split.Rule = "CT",
#   split.Honest = TRUE,
#   cv.option = "CT",
#   cv.Honest = TRUE,
#   split.Bucket = TRUE,
#   bucketNum = 40,
#   minsize = 60,
#   cp = 0
# )



# b
m2 |> rpart.plot()
```

The population with the biggest treatment effect are students with 
- no first generation student
- have a financial need of at least 68
- have a gpa below 2.9

The population with the smallest treatment effect are students with
- the first generation
- sat score of less than 932
- distance to college of more than 15 miles
- gpa below 2.9

c. We included all variables, so we suppose all variables have an influence on the outcome, but that is not necessarily so.


## Task 5
Given potential concerns of selection bias, you shall next examine the potential imbalance of treated and untreated observations inside different leaves. To do so, please follow these steps (Hint: various code-chunks are provided that may be helpful):

a. Estimate a propensity score model—a standard logistic regression model using glm() with family=binomial()—predicting the treatment variable based on the confounders. Specify type="response" in the predict() function.

b. Calculate the mean and standard deviation of the propensity scores within each leaf and treatment group combination. (Hint: use $where to extract leaf assignments)

c. Based on the mean and standard deviation, calculate the standardized difference in means measure within each leaf. What do these indicate about your results in #4?

```{r}
#| label: Task 2.5
#| fig.align: center
#| fig-width: 7
#| fig-height: 4


# a
m_ps <- glm(scholarship ~ gpa + parental_income +
              first_generation + sat_score +
              distance_to_college + financial_need,
            family = "binomial",
            data = d)

d$ps_hat <- m_ps |> 
  predict(type = "response") |> 
  as.numeric()

# b
d <- data.table(d)

d$leaf <- factor(m2$where)

leaf_group_long <- d[, .(
  n       = .N,
  mean_ps = mean(ps_hat),
  sd_ps   = sd(ps_hat)
  ), by = .(leaf, scholarship)]

leaf_group_stats <- data.table::dcast(
  leaf_group_long,
  leaf ~ scholarship,
  value.var = c("n", "mean_ps", "sd_ps"),
  fill = NA_real_
  )

# c
leaf_balance <- copy(leaf_group_stats)[
  , `:=`(
    SMD_ps = {
      denom <- sqrt((sd_ps_1^2 + sd_ps_0^2)/2)
      ifelse(is.finite(denom) & denom > 0,
             abs(mean_ps_1 - mean_ps_0) / denom, NA_real_)
      }
    )
  ][order(-SMD_ps)]

leaf_balance |> kable()

leaf_balance$SMD_ps |> 
  barplot(main = "Propensityscore difference per leaf")
```

The in c calculated standardized mean difference of propensity scores indicates how comparable the groups in each leaf are. 
The propensity score is calculated in the beginning to assess similarity of observations. 
the `mean_ps_0` and `mean_ps_1` variable are the mean propensity scores for the treatment and control group. Together with the standard deviation of each leaf it is possible to calculate the difference between both groups for each leaf. Therefore leafs with a high balance (small SMD) are well comparable, while leafs with high imbalance (high SMD) differ in their treatment probability. 

Only 4 leafs have a propensity score difference lower than 0.2! Most leafs do not allow a sufficient comparison of groups!

## Task 6
Given your findings in the previous task, you shall next do a causal tree analysis wherein you incorporate inverse probability weighting. To do so, please do the following:

a. Refit the causal tree using causalTree() with same specifications as in #4, and add the weights argument set to 1/p for treated units and 1/(1-p) for control units, where p is the predicted propensity score (see code chunk below for how you could do this). This incorporates IPW into the tree.

b. Assess the balance for this tree in the same way you did in #5 (but you can skip the first step which estimates the propensity score model). Did the balance improve in comparison to #4?

```{r}
#| label: Task 2.6
#| fig.align: center
#| fig-width: 7
#| fig-height: 4

# create weights
d$w_ipw <- ifelse(test = d$scholarship == 1,
                  yes = 1/pmax(d$ps_hat, 0.02),
                  no = 1/pmax(1-d$ps_hat, 0.02))


# fit tree 
set.seed(5)

if(file.exists("task2_6.rds")){
  m3 <- readRDS("task2_6.rds")
  } else {
    
    m3 <- causalTree(
      completed ~ gpa + parental_income +
             first_generation + sat_score + distance_to_college + financial_need,
      data      = d,
      treatment = d$scholarship,
      weights = w_ipw,
      split.Rule = "CT", cv.option = "CT", 
      split.Honest = T, split.Bucket = T, 
      minsize = 60, cp = 0.000043, bucketNum = 40
    )
    
    saveRDS(m3, "task2_6.rds")
}

# b.
d$leaf_2 <- factor(m3$where)

leaf_group_long_2 <- d[, .(
  n       = .N,
  mean_ps = weighted.mean(ps_hat, w = d$w_ipw),
  sd_ps   = sd(ps_hat)
  ), by = .(leaf_2, scholarship)]

leaf_group_stats_2 <- data.table::dcast(
  leaf_group_long_2,
  leaf_2 ~ scholarship,
  value.var = c("n", "mean_ps", "sd_ps"),
  fill = NA_real_
  )

leaf_balance_2 <- copy(leaf_group_stats_2)[
  , `:=`(
    SMD_ps = {
      denom <- sqrt((sd_ps_1^2 + sd_ps_0^2)/2)
      ifelse(is.finite(denom) & denom > 0,
             abs(mean_ps_1 - mean_ps_0) / denom, NA_real_)
      }
    )
  ][order(-SMD_ps)]

leaf_balance_2 |> kable()

leaf_balance_2$SMD_ps |> 
  barplot(main = "Propensityscore difference per leaf")
```

The tree is waayyy more balanced now :)


c. Visualize the tree and provide an interpretation of its structure, highlighting what you think is interesting in it. Are the conclusions you draw from this tree different from those in #4? What does this suggest about the findings in #4?

```{r}
#| label: Task 2.6c
#| fig.align: center
#| fig-width: 15
#| fig-height: 7
#| fig-caption: "Causal Tree"

m3 |> rpart.plot()
```

The population with the biggest treatment effect are students with:
- first generation student (first_generation = 1)
- GPA of at least 2.9
- financial need of at least 71
- parental income of at least 60
- SAT score of at least 1029
- GPA below 2.5

The population with the smallest treatment effect are students with:
- no first generation student (first_generation = 0)
- SAT score below 1107
- SAT score below 1005
- GPA below 2.6


d. To get a sense of the subgroups contained within each leaf (of interest), please describe its average properties in terms of all the input variables (except treatment).

```{r}
#| label: Task 2.6d1
#| fig.align: center
#| fig-width: 9
#| fig-height: 6

leafeffects <- copy(d)[, leaf := m3$where][,
  .(n=.N, 
    eff = mean(completed[scholarship==1]) - mean(completed[scholarship==0])),
  by = leaf][order(-eff)]


pca <- d[, c("completed", "gpa", "parental_income", "first_generation",
             "sat_score", "distance_to_college", "financial_need")
         ] |>
  prcomp(scale. = TRUE, center = TRUE)

d$PC1 <- pca$x[, 1]
d$PC2 <- pca$x[, 2]

leaf_summary <- aggregate(
  cbind(PC1, PC2, completed, gpa, parental_income, first_generation,
        sat_score, distance_to_college, financial_need) ~ leaf_2,
  data = d,
  FUN = mean
)

#print(leaf_summary)

leaf_pcs <- aggregate(cbind(PC1, PC2) ~ leaf_2, data = d, FUN = mean)

leaf_pcs <- merge(leaf_pcs, leafeffects[, c("leaf", "eff")], 
                     by.x = "leaf_2", by.y = "leaf")

plot(leaf_pcs$PC1, leaf_pcs$PC2, 
     pch = 19, cex = 2, 
     col = hcl.colors(nrow(leaf_pcs), palette = "Blue-Red 3")[rank(leaf_pcs$eff)],
     xlab = "PC1", ylab = "PC2",
     main = "Leaf Centroids in PC Space (colored by effect size)")
text(leaf_pcs$PC1, leaf_pcs$PC2, labels = leaf_pcs$leaf_2, pos = 2)
legend("topleft", pch = 19, title = "Effect Size",
       legend = sprintf("%.2f", 
                        quantile(leaf_pcs$eff, 
                                 probs = c(0, 0.25, 0.5, 0.75, 1))),
       col = hcl.colors(5, palette = "Blue-Red 3", 
                        rev = TRUE),
       cex = 0.8, bty = "n")
```

```{r}
#| label: Task 2.6d2
#| fig.align: center
#| fig-width: 12
#| fig-height: 5

par(mfrow = c(1, 2), mar = c(4, 10, 3, 1))

# PC1
barplot(sort(pca$rotation[, 1]), horiz = TRUE, las = 1,
        col = hcl.colors(7, palette = "Blue-Red2", rev = T), 
        main = "PC1 Loadings")
abline(v = 0, lty = 2)

# PC2
barplot(sort(pca$rotation[, 2]), horiz = TRUE, las = 1,
        col = hcl.colors(7, palette = "Blue-Red2", rev = T), 
        main = "PC2 Loadings")
abline(v = 0, lty = 2)

par(mfrow = c(1, 1))
```

The PCA reveals that many leafs are associated with high PC1 values and most are centered for PC2. PC1 is representing fincancial need where students with rich parents are not in financial need. PC2 is making a distinction between the distance to the college were students who live closer get better grades. We can see that leaf 4 represents the students with rich parents that do not have much better grades than the mean of the students. Most students have above average financial need and no rich parents. Cluster 11 and 18 are students who live far apart and are getting bad grades, while leaf 31 are students that get the best grades. They are also in mediocre financial need.

e. How do these results map onto your findings in the first analysis in #1? Do you find that the variables suggested are most important indeed are so? What would you say to your co-author?

My coauthor seems to be right! Financial need is the most important divider between groups of students! nice catch!

# Part 3

In this part, you will continue working with the scholarship dataset from Part 2.


##  Task 1

In this last part, you shall continue with the exploration of heterogeneous treatment effects. But instead of standard OLS and causal trees, you shall use causal forests. Before doing so, please answer the question: why is it not necessary to include inverse probability weighting in causal forest, like we did for trees?

The original idea of the IPW was to balance the trees across treated and untreated population. We wanted to make the leafs as comparable as possible. My including probability weighting we limited the information/influence of less comparable subgroups in the resulting tree. Causal forests algorithms include augmented inverse probability weighting (AIPW) by default. [https://chenxing.space/blog/a-walkthrough-of-how-causal-forest-works/#causal-forest](https://chenxing.space/blog/a-walkthrough-of-how-causal-forest-works/#causal-forest)


## Task 2

2. Now, run a causal forest analysis following these steps:

a. Estimate the causal forest using the function causal_forest() from the grf package, specifying the input arguments as follows: X (matrix of covariates), Y (outcome), W (treatment), num.trees=2000, and honesty=TRUE. Set a seed for reproducibility. Report the average treatment effect using average_treatment_effect().

b. Examine which variables were most important to account for the heterogeneity in treatment effect by using the function variable_importance(). Make a bar chart and interpret. Does this result line up with your findings using causal tree—do the most important variables here overlap with those showing up in the best causal tree?

c. For the two variables you identified as most important, please examine how the effects vary along these dimensions. To do so, divide into quintiles of these variables (if continuous) and calculate average treatment effects for each subcategory separately. Plot how the treatment effect varies across quintiles and interpret. Does this result provide additional information to what you could infer from the causal tree?


```{r}
#|label: Task 3.2

d <- data.frame(d)

confounders <- c("gpa", "parental_income", "first_generation",
                 "sat_score", "distance_to_college", "financial_need")

# a

set.seed(5)
if(file.exists("task3_2.rds")){
  m4 <- readRDS("task3_2.rds")
  } else {
    
    m4 <- causal_forest(d[,confounders] |> 
                      as.matrix(),
                    d$completed, 
                    d$scholarship, 
                    num.trees = 2000, 
                    honesty = TRUE)
    
    saveRDS(m4, "task3_2.rds")
}

average_treatment_effect(m4)
```

```{r}
#| label: Task3.2b

# b
par(mar = c(5, 10, 4, 2))

variable_importance(m4) |>
  setNames(confounders) |>
  sort() |>
  as.table() |>
  barplot(horiz = TRUE, 
          las = 1,
          col = hcl.colors(6,rev = T),
          main = "Variable Importance Plot")
```

This result aligns with the causal tree, since the first split in the tree was always `first_generation`.



```{r}
#| label: Task3.2c
#| fig.align: center
#| fig-width: 12
#| fig-height: 5

# Get predicted treatment effects
d$tau_hat <- predict(m4)$predictions

# Get top 2 most important variables
var_imp <- variable_importance(m4)
names(var_imp) <- confounders
top_vars <- names(sort(var_imp, decreasing = TRUE)[1:2])


# plot
par(mfrow = c(1, 2),
    mar = c(3, 3, 3, 3))

barplot(tapply(d$tau_hat, d$first_generation, mean),
        names.arg = c("Not First-Gen", "First-Gen"),
        col = hcl.colors(2,palette = "SunsetDark"),
        main = "ATE by First Generation",
        ylab = "ATE")

d$quintile <- cut(d[[top_vars[2]]], 
                  breaks = quantile(d[[top_vars[2]]], 
                                    probs = seq(0, 1, 0.2)),
                  include.lowest = T,
                  labels = paste0("Q", 1:5))

barplot(tapply(d$tau_hat, d$quintile, mean),
        col  = hcl.colors(5,palette = "SunsetDark"),
        main = paste("ATE by", top_vars[2], "Quintiles"),
        ylab = "ATE",
        xlab = "Quintiles")
```

Yes, the causal forest provides important additional insights. While the causal tree identified first-generation status and SAT scores as key moderators through discrete splits, the forest reveals the magnitude and continuous nature of these effects. First-generation students show treatment effects 6 times larger than non-first-generation students (0.25 vs 0.04). SAT scores show a smooth, monotonic increase in treatment effects across quintiles rather than discrete jumps, suggesting scholarships benefit students across the entire SAT distribution with gradually increasing returns. This continuous pattern would be obscured by the tree's binary splits.