---
title: "Lab 1"
subtitle: ""
author: "Thomas Haase"
date: last-modified
date-format: long
 
execute:
  warning: false
  echo: true
  error: false  
format: 
  pdf: 
    toc: true
    geometry:
     - top=2cm
     - left=2cm
     - right=2cm
     - bottom=3cm
    fig-pos: 'h'
    fig-cap-location: bottom
    tbl-cap-location: bottom
    include-in-header:
    - text: |
        \usepackage{placeins}
number-sections: true
#bibliography: citations.bib
editor_options: 
  chunk_output_type: console
---

---

```{r}
#| warning: false
#| echo: false
#| error: false
#| output: false

setwd("~/Github/ML-Labs")

library(data.table)
library(caret)
library(glmnet)
```


# Part 1 - Bernie Sanders and Donald Trump tweets 
## Task 1.1
```{r}
data <- fread(file = './1/trumpbernie.csv')

data[1:5,20:25]
nrow(data)
ncol(data)
```

High-dimensionality describes a dataset where the number of variables is large relative to the number of observations. Since the columncount of the data is larger than the number of observations the dataset is highdimensional. A logistic regression would produce a perfect fit, which means that each observation can be completely explained. Since in this case also the "noise" (<tiny details we are not interested in>) is part of the data modeling, the model becomes too flexible. This is also called overfitting. 

## Task 1.2
### a)
```{r}
glm <- glm(trump_tweet ~ .,
           data = data,
           family = "binomial")

coef(glm)[1000:1050]
```

A lot of coefficients are NA/not defined values. The model only fits the parameters it needs to reach a perfect fit because of the high dimensionality and the rest are set to NA. The model tries to improve the fit until it can explain every observation. 

### b)
```{r}
# Extract predictions on training data & observed values
comparison_df <- data.frame(train_predictions=glm$fitted.values,
observed=glm$y)
# Apply prediction threshold
comparison_df$train_predictions<-ifelse(comparison_df$train_predictions>=0.5,
yes = 1,
no = 0)
# Compute accuracy (scale: 0-1, 0=0%, 1=100%)
nrow(comparison_df[comparison_df$train_predictions==comparison_df$observed,]) /
nrow(comparison_df)
```
As theorized in a) the model reached a perfect fit. 

## Task 1.3
```{r}
tc <- trainControl(method = 'cv', number = 3)
glm_cv <- caret::train(as.factor(trump_tweet) ~ .,
                       data = data,
                       method = "glm",
                       family = "binomial",
                       trControl = tc)

glm_cv$results
```

The accuracy is of 50%. Accuracy measures the amount of in/correctly predicted predictions. The model is pretty useless. 
# FINISH THIS INTERPRETATION


## Task 1.4
```{r}
glm_ridge <- cv.glmnet(x = as.matrix(data[,-"trump_tweet"]), 
                       y = data$trump_tweet,
                       nfolds = 5,                # Number CV-folds
                       standardize = TRUE,        # Standardize X
                       family = "binomial",       # Outcome binary --> logit/binomial
                       alpha = 0,                 # alpha=0 --> ridge, for Lasso set it to 1
                       type.measure = "class")    # measure performance in terms of accuracy


glm_ridge
```
1 - Measure (0.092) is the accuracy = 0.90728

## Task 1.5
```{r}
plot(glm_ridge, sign.lambda = 1) 
```


## Task 1.6
```{r}
data.table(word = rownames(coef(glm_ridge, s = "lambda.min")),
           coef = coef(glm_ridge, s = "lambda.min")[,1])[order(coef,decreasing = T)]
```

