saveRDS(model_forest, file = "model_forest.rds")
} else {
model_forest <- readRDS("model_forest.rds")
}
model_forest$results[,c("mtry", "Accuracy", "AccuracySD")] |> round(3) |> kable()
model_forest$results[,c("mtry", "Accuracy", "AccuracySD")] |>
model_forest$results[,c("mtry", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
xlab = "Complexity Parameter Alpha",
main = "Accuracy of Decision Tree")
model_forest$results[,c("mtry", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
xlab = "Complexity Parameter Alpha",
main = "Accuracy of Decision Tree")
box("plot", bty = "l", lwd = 2)
abline(v = model_tree$results$cp[which.max(model_tree$results$Accuracy)], col = "red", lty = 2)
abline(v = model_forest$results$mtry[which.max(model_forest$results$Accuracy)], col = "red", lty = 2)
model_forest$results[,c("mtry", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
xlab = "Complexity Parameter Alpha",
main = "Accuracy of Decision Tree")
box("plot", bty = "l", lwd = 2)
abline(v = model_forest$results$mtry[which.max(model_forest$results$Accuracy)], col = "red", lty = 2)
model_forest$results[,c("mtry", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log = "x",
xlab = "Complexity Parameter Alpha",
main = "Accuracy of Decision Tree")
box("plot", bty = "l", lwd = 2)
abline(v = model_forest$results$mtry[which.max(model_forest$results$Accuracy)], col = "red", lty = 2)
model_forest$results[,c("mtry", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log = "x",
xlab = "Log(mtry): Number of Variables considered for each tree",
main = "Accuracy of Random Forest")
model_forest$results[,c("mtry", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log = "x",
xlab = "Log(mtry): Number of Variables considered for each tree",
main = "Accuracy of Random Forest")
box("plot", bty = "l", lwd = 2)
abline(v = model_forest$results$mtry[which.max(model_forest$results$Accuracy)], col = "red", lty = 2)
model_forest$results[,c("mtry", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log = "x",
xlab = "Log(mtry)\nNumber of Variables considered for each tree",
main = "Accuracy of Random Forest")
box("plot", bty = "l", lwd = 2)
abline(v = model_forest$results$mtry[which.max(model_forest$results$Accuracy)], col = "red", lty = 2)
sqrt(nrow(data))
model_forest$results$Accuracy[which(model_forest$results$mtry == 1)]
model_forest$results$Accuracy[which(model_forest$results$mtry == 1)] |> round(2)
data <- fread("Kaggle_Social_Network_Ads.csv")[, `:=`(
user_id = NULL,
Purchased = as.factor(Purchased)
)]
tc <- caret::trainControl(method = "cv", number = 10)
model_knn <- caret::train(Purchased ~ .,
data = data,
preProcess = c("center","scale"),
tuneGrid = expand.grid(k = c(1:10,seq(15,25,5),50)),
method = "knn",
trControl = tc)
ggplot(model_knn$results, aes(x = k, y = Accuracy)) +
geom_line() +
geom_point() +
geom_vline(xintercept = model_knn$results$k[which.max(model_knn$results$Accuracy)],
color = "red") +
labs(title = "Accuracy of K-Nearest Neighbors model",
subtitle = "Trained with 10-fold cross-validation") +
scale_x_continuous(breaks = c(1:10,seq(15,25,5),50)) +
scale_y_continuous(breaks = seq(0.8,1,0.01)) +
theme_minimal()
ggplot(model_knn$results, aes(x = k, y = Accuracy)) +
geom_line() +
geom_point() +
geom_vline(xintercept = model_knn$results$k[which.max(model_knn$results$Accuracy)],
color = "red") +
labs(title = "Accuracy of K-Nearest Neighbors model",
subtitle = "Trained with 10-fold cross-validation") +
scale_x_log10(breaks = c(1:10,seq(15,25,5),50)) +
scale_y_continuous(breaks = seq(0.8,1,0.01)) +
theme_minimal()
data <- fread("Kaggle_Social_Network_Ads_Augmented.csv")[, `:=`(
user_id = NULL,
Purchased = as.factor(Purchased)
)]
tc <- caret::trainControl(method = "cv", number = 10)
model_knn <- caret::train(Purchased ~ .,
data = data,
preProcess = c("center","scale"),
tuneGrid = expand.grid(k = c(1:10,seq(15,25,5),50)),
method = "knn",
trControl = tc)
ggplot(model_knn$results, aes(x = k, y = Accuracy)) +
geom_line() +
geom_point() +
geom_vline(xintercept = model_knn$results$k[which.max(model_knn$results$Accuracy)],
color = "red") +
labs(title = "Accuracy of K-Nearest Neighbors model",
subtitle = "Trained with 10-fold cross-validation") +
scale_x_log10(breaks = c(1:10,seq(15,25,5),50)) +
scale_y_continuous(breaks = seq(0,1,0.01)) +
theme_minimal()
model_tree <- caret::train(Purchased ~ .,
data = data,
tuneGrid = expand.grid(cp = c(0, 0.01, 0.025, 0.05,
0.10, 0.25, 0.5, 1)),
control = rpart.control(minbucket = 10),
method = "rpart",
trControl = tc)
model_tree$results[,c("cp", "Accuracy", "AccuracySD")] |> round(3) |> kable()
model_tree$results[,c("cp", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
xlab = "Complexity Parameter Alpha",
main = "Accuracy of Decision Tree")
box("plot", bty = "l", lwd = 2)
abline(v = model_tree$results$cp[which.max(model_tree$results$Accuracy)], col = "red", lty = 2)
model_tree$results[,c("cp", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log = "x",
xlab = "Complexity Parameter Alpha",
main = "Accuracy of Decision Tree")
box("plot", bty = "l", lwd = 2)
abline(v = model_tree$results$cp[which.max(model_tree$results$Accuracy)], col = "red", lty = 2)
abline(v = model_tree$results$cp[which.max(model_tree$results$Accuracy)], col = "red", lty = 2)
model_tree$results[,c("cp", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
xlab = "Complexity Parameter Alpha",
main = "Accuracy of Decision Tree")
box("plot", bty = "l", lwd = 2)
abline(v = model_tree$results$cp[which.max(model_tree$results$Accuracy)], col = "red", lty = 2)
model_tree$results[,c("cp", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log = "x",
xlab = "Complexity Parameter Alpha",
main = "Accuracy of Decision Tree")
box("plot", bty = "l", lwd = 2)
abline(v = model_tree$results$cp[which.max(model_tree$results$Accuracy)], col = "red", lty = 2)
model_tree$results[,c("cp", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
sqrt = "x",
xlab = "Complexity Parameter Alpha",
main = "Accuracy of Decision Tree")
box("plot", bty = "l", lwd = 2)
abline(v = model_tree$results$cp[which.max(model_tree$results$Accuracy)], col = "red", lty = 2)
model_tree$results[,c("cp", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log = "x",
xlab = "Complexity Parameter Alpha",
main = "Accuracy of Decision Tree")
box("plot", bty = "l", lwd = 2)
abline(v = model_tree$results$cp[which.max(model_tree$results$Accuracy)], col = "red", lty = 2)
model_tree$results[,c("cp", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log = "x",
xlab = "Log(Alpha)\nComplexity Parameter Alpha",
main = "Accuracy of Decision Tree")
box("plot", bty = "l", lwd = 2)
abline(v = model_tree$results$cp[which.max(model_tree$results$Accuracy)], col = "red", lty = 2)
box("plot", bty = "l", lwd = 2)
model_tree$results[,c("cp", "Accuracy")] |>
round(3) |>
plot(log10(res$cp + 1e-6), round(res$Accuracy, 3),
type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
xlab = "Log(Alpha)\nComplexity Parameter Alpha",
main = "Accuracy of Decision Tree")
model_tree$results[,c("cp", "Accuracy")] |>
round(3) |>
plot(log10(res$cp + 1e-6), round(res$Accuracy, 3),
type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
xlab = "Log(Alpha)\nComplexity Parameter Alpha",
main = "Accuracy of Decision Tree")
model_tree$results[,c("cp", "Accuracy")]
model_tree$results[, cp := ifelse(cp == 0, 1e-6, cp)][, .(cp, Accuracy)] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log = "x",
xlab = "Log(Alpha)\nComplexity Parameter Alpha",
main = "Accuracy of Decision Tree")
model_tree$results[, cp := fifelse(cp == 0, 1e-6, cp)][, .(cp, Accuracy)] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log = "x",
xlab = "Log(Alpha)\nComplexity Parameter Alpha",
main = "Accuracy of Decision Tree")
model_tree$results[,c("cp", "Accuracy")][] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log1p = "x",
xlab = "Log(Alpha)\nComplexity Parameter Alpha",
main = "Accuracy of Decision Tree")
model_tree$results[,c("cp", "Accuracy")][] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log1p = "x",
xlab = "Log(Alpha)\nComplexity Parameter Alpha",
main = "Accuracy of Decision Tree")
box("plot", bty = "l", lwd = 2)
abline(v = model_tree$results$cp[which.max(model_tree$results$Accuracy)], col = "red", lty = 2)
model_tree$results[,c("cp", "Accuracy")][] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log = "x",
xlab = "Log(Alpha)\nComplexity Parameter Alpha",
main = "Accuracy of Decision Tree")
box("plot", bty = "l", lwd = 2)
abline(v = model_tree$results$cp[which.max(model_tree$results$Accuracy)], col = "red", lty = 2)
model_tree$results[,c("cp", "Accuracy")][cp := ifelse(cp == 1, cp + 0.00000001,cp)] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log = "x",
xlab = "Log(Alpha)\nComplexity Parameter Alpha",
main = "Accuracy of Decision Tree")
model_tree$results[, cp := ifelse(cp == 1, cp + 1e-8, cp)][,c("cp", "Accuracy")] |>
#model_tree$results[,c("cp", "Accuracy")][cp := ifelse(cp == 1, cp + 0.00000001,cp)] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log = "x",
xlab = "Log(Alpha)\nComplexity Parameter Alpha",
main = "Accuracy of Decision Tree")
setDT(model_tree$results)
model_tree$results[, cp := ifelse(cp == 1, cp + 1e-8, cp)][,c("cp", "Accuracy")] |>
#model_tree$results[,c("cp", "Accuracy")][cp := ifelse(cp == 1, cp + 0.00000001,cp)] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log = "x",
xlab = "Log(Alpha)\nComplexity Parameter Alpha",
main = "Accuracy of Decision Tree")
setDT(model_tree$results)
model_tree$results[, cp := ifelse(cp == 1, cp + 1e-8, cp)][,c("cp", "Accuracy")] |>
#model_tree$results[,c("cp", "Accuracy")][cp := ifelse(cp == 1, cp + 0.00000001,cp)] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log = "x",
xlab = "Log(Alpha)\nComplexity Parameter Alpha",
main = "Accuracy of Decision Tree")
box("plot", bty = "l", lwd = 2)
abline(v = model_tree$results$cp[which.max(model_tree$results$Accuracy)], col = "red", lty = 2)
box("plot", bty = "l", lwd = 2)
abline(v = model_tree$results$cp[which.max(model_tree$results$Accuracy)], col = "red", lty = 2)
model_tree$results$cp[which.max(model_tree$results$Accuracy)]
```{r}
model_tree$results[,c("cp", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
xlab = "Log(Alpha)\nComplexity Parameter Alpha",
main = "Accuracy of Decision Tree")
box("plot", bty = "l", lwd = 2)
abline(v = model_tree$results$cp[which.max(model_tree$results$Accuracy)], col = "red", lty = 2)
model_tree$results[,c("cp", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
xlab = "Log(Alpha)\nComplexity Parameter Alpha",
main = "Accuracy of Decision Tree")
box("plot", bty = "l", lwd = 2)
abline(v = model_tree$results$cp[which.max(model_tree$results$Accuracy)], col = "red", lty = 2)
tc <- caret::trainControl(method = "cv", number = 10)
model_knn <- caret::train(Purchased ~ .,
data = data,
preProcess = c("center","scale"),
tuneGrid = expand.grid(k = c(1:10,seq(15,25,5),50)),
method = "knn",
trControl = tc)
ggplot(model_knn$results, aes(x = k, y = Accuracy)) +
geom_line() +
geom_point() +
geom_vline(xintercept = model_knn$results$k[which.max(model_knn$results$Accuracy)],
color = "red") +
labs(title = "Accuracy of K-Nearest Neighbors model",
subtitle = "Trained with 10-fold cross-validation") +
scale_x_log10(breaks = c(1:10,seq(15,25,5),50)) +
scale_y_continuous(breaks = seq(0,1,0.01)) +
theme_minimal()
tc <- caret::trainControl(method = "cv", number = 10)
model_knn <- caret::train(Purchased ~ .,
data = data,
preProcess = c("center","scale"),
tuneGrid = expand.grid(k = c(1:10,seq(15,25,5),50)),
method = "knn",
trControl = tc)
ggplot(model_knn$results, aes(x = k, y = Accuracy)) +
geom_line() +
geom_point() +
geom_vline(xintercept = model_knn$results$k[which.max(model_knn$results$Accuracy)],
color = "red") +
labs(title = "Accuracy of K-Nearest Neighbors model",
subtitle = "Trained with 10-fold cross-validation",
x = "Log10(k)") +
scale_x_log10(breaks = c(1:10,seq(15,25,5),50)) +
scale_y_continuous(breaks = seq(0,1,0.01)) +
theme_minimal()
# Chunk 1
#| warning: false
#| echo: false
#| error: false
#| output: false
setwd("~/Github/ML-Labs/2")
set.seed(2)
library(data.table)
library(caret)
library(rpart)
library(rpart.plot)
library(kableExtra)
# Chunk 2
data <- fread("Kaggle_Social_Network_Ads.csv")[, `:=`(
user_id = NULL,
Purchased = as.factor(Purchased)
)]
tc <- caret::trainControl(method = "cv", number = 10)
model_knn <- caret::train(Purchased ~ .,
data = data,
preProcess = c("center","scale"),
tuneGrid = expand.grid(k = c(1:10,seq(15,25,5),50)),
method = "knn",
trControl = tc)
ggplot(model_knn$results, aes(x = k, y = Accuracy)) +
geom_line() +
geom_point() +
geom_vline(xintercept = model_knn$results$k[which.max(model_knn$results$Accuracy)],
color = "red") +
labs(title = "Accuracy of K-Nearest Neighbors model",
subtitle = "Trained with 10-fold cross-validation") +
scale_x_log10(breaks = c(1:10,seq(15,25,5),50)) +
scale_y_continuous(breaks = seq(0.8,1,0.01)) +
theme_minimal()
tc <- caret::trainControl(method = "cv", number = 10)
model_knn <- caret::train(Purchased ~ .,
data = data,
preProcess = c("center","scale"),
tuneGrid = expand.grid(k = c(1:10,seq(15,25,5),50)),
method = "knn",
trControl = tc)
ggplot(model_knn$results, aes(x = k, y = Accuracy)) +
geom_line() +
geom_point() +
geom_vline(xintercept = model_knn$results$k[which.max(model_knn$results$Accuracy)],
color = "red") +
labs(title = "Accuracy of K-Nearest Neighbors model",
subtitle = "Trained with 10-fold cross-validation",
x = "Log10(k)") +
scale_x_log10(breaks = c(1:10,seq(15,25,5),50)) +
scale_y_continuous(breaks = seq(0.8,1,0.01)) +
theme_minimal()
varimp <- data.table(var = rownames(caret::varImp(model_tree$finalModel)),
imp = caret::varImp(model_tree$finalModel)$Overall)[order(imp,decreasing = T)]
varimp[,imp := imp / max(imp)] |>
ggplot(aes(y=reorder(var, imp),x=imp)) +
geom_segment(aes(x = 0, xend = imp,
y = var, yend = var),
color = "#333333") +
geom_point(size=3) +
labs(title = "Variable Importance",
subtitle = "Best fitting decision tree",
x = "Relative Variable Importance",
y = "Variables") +
theme_minimal()
# Chunk 1
#| warning: false
#| echo: false
#| error: false
#| output: false
setwd("~/Github/ML-Labs/2")
set.seed(2)
library(data.table)
library(caret)
library(rpart)
library(rpart.plot)
library(kableExtra)
# Chunk 2
data <- fread("Kaggle_Social_Network_Ads.csv")[, `:=`(
user_id = NULL,
Purchased = as.factor(Purchased)
)]
# Chunk 3
tc <- caret::trainControl(method = "cv", number = 10)
model_knn <- caret::train(Purchased ~ .,
data = data,
preProcess = c("center","scale"),
tuneGrid = expand.grid(k = c(1:10,seq(15,25,5),50)),
method = "knn",
trControl = tc)
ggplot(model_knn$results, aes(x = k, y = Accuracy)) +
geom_line() +
geom_point() +
geom_vline(xintercept = model_knn$results$k[which.max(model_knn$results$Accuracy)],
color = "red") +
labs(title = "Accuracy of K-Nearest Neighbors model",
subtitle = "Trained with 10-fold cross-validation",
x = "Log10(k)") +
scale_x_log10(breaks = c(1:10,seq(15,25,5),50)) +
scale_y_continuous(breaks = seq(0.8,1,0.01)) +
theme_minimal()
# Chunk 4
data <- fread("Kaggle_Social_Network_Ads_Augmented.csv")[, `:=`(
user_id = NULL,
Purchased = as.factor(Purchased)
)]
# Chunk 5
tc <- caret::trainControl(method = "cv", number = 10)
model_knn <- caret::train(Purchased ~ .,
data = data,
preProcess = c("center","scale"),
tuneGrid = expand.grid(k = c(1:10,seq(15,25,5),50)),
method = "knn",
trControl = tc)
ggplot(model_knn$results, aes(x = k, y = Accuracy)) +
geom_line() +
geom_point() +
geom_vline(xintercept = model_knn$results$k[which.max(model_knn$results$Accuracy)],
color = "red") +
labs(title = "Accuracy of K-Nearest Neighbors model",
subtitle = "Trained with 10-fold cross-validation",
x = "Log10(k)") +
scale_x_log10(breaks = c(1:10,seq(15,25,5),50)) +
scale_y_continuous(breaks = seq(0,1,0.01)) +
theme_minimal()
# Chunk 6
model_tree <- caret::train(Purchased ~ .,
data = data,
tuneGrid = expand.grid(cp = c(0, 0.01, 0.025, 0.05,
0.10, 0.25, 0.5, 1)),
control = rpart.control(minbucket = 10),
method = "rpart",
trControl = tc)
model_tree$results[,c("cp", "Accuracy", "AccuracySD")] |> round(3) |> kable()
# Chunk 7
model_tree$results[,c("cp", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
xlab = "Complexity Parameter Alpha",
main = "Accuracy of Decision Tree")
box("plot", bty = "l", lwd = 2)
abline(v = model_tree$results$cp[which.max(model_tree$results$Accuracy)], col = "red", lty = 2)
# Chunk 8
rpart.plot(model_tree$finalModel)
# Chunk 9
varimp <- data.table(var = rownames(caret::varImp(model_tree$finalModel)),
imp = caret::varImp(model_tree$finalModel)$Overall)[order(imp,decreasing = T)]
varimp[,imp := imp / max(imp)] |>
ggplot(aes(y=reorder(var, imp),x=imp)) +
geom_segment(aes(x = 0, xend = imp,
y = var, yend = var),
color = "#333333") +
geom_point(size=3) +
labs(title = "Variable Importance",
subtitle = "Best fitting decision tree",
x = "Relative Variable Importance",
y = "Variables") +
theme_minimal()
# Chunk 10
rm(list = ls())
data <- fread("trumpbernie2.csv")[, `:=`(
trump_tweet = as.factor(trump_tweet)
)]
set.seed(2025)
model_tree <- caret::train(trump_tweet ~ .,
data = data,
method = "rpart",
tuneGrid  = expand.grid(cp = c(0, 0.001, 0.01, 0.1, 0.25)),
control   = rpart.control(minbucket = 10),
trControl = trainControl(method = "cv", number = 5))
model_tree$results[,c("cp", "Accuracy", "AccuracySD")] |> round(3) |> kable()
rpart.plot(model_tree$finalModel)
varimp <- data.table(var = rownames(caret::varImp(model_tree$finalModel)),
imp = caret::varImp(model_tree$finalModel)$Overall)[imp != 0][order(imp,decreasing = T)]
varimp[,imp := imp / max(imp)][, var := sub("_freq$", "", var)] |>
ggplot(aes(y=reorder(var, imp),x=imp)) +
geom_segment(aes(x = 0, xend = imp,
y = var, yend = var),
color = "#333333") +
geom_point(size=3) +
labs(title = "Variable Importance",
subtitle = "Best fitting decision tree",
x = "Relative Variable Importance",
y = "Variables") +
theme_minimal()
if(!file.exists("model_forest.rds")){
set.seed(2025)
model_forest <- caret::train(trump_tweet ~ .,
data = data,
method = "rf",
ntree = 200,
tuneGrid  = expand.grid(mtry = c(1, 5, 10, 25, 50, 200)),
trControl = trainControl(method = "cv", number = 5))
saveRDS(model_forest, file = "model_forest.rds")
} else {
model_forest <- readRDS("model_forest.rds")
}
model_forest$results[,c("mtry", "Accuracy", "AccuracySD")] |> round(3) |> kable()
model_forest$results[,c("mtry", "Accuracy")] |>
round(3) |>
plot(type = "b", bty = "n", pch = 20, cex = 1.5, las = 1,
log = "x",
xlab = "Log(mtry)\nNumber of Variables considered for each tree",
main = "Accuracy of Random Forest")
box("plot", bty = "l", lwd = 2)
abline(v = model_forest$results$mtry[which.max(model_forest$results$Accuracy)], col = "red", lty = 2)
